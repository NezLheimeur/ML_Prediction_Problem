{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KaggleInClass project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this competition, you will be challenged to predict the number of shares an article will get on social media, from the article's topic, length, day of publication, and many other features.\n",
    "\n",
    "You are given labels, that is, number of shares, for 5000 of these articles; your task is to predict labels for the remaining 2000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# load useful libraries\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from pandas import plotting\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data upload\n",
    "First let's upload the different data infomations. \n",
    "We will upload the features, the training data in \"feature_data\", the training data labels in \"target_data\", and the testing data in \"test_data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_words_title  Number of words in the article's titles\n",
      "nb_words_content  Number of words in the article\n",
      "pp_uniq_words  Proportion of unique words in the article\n",
      "pp_stop_words  Proportion of stop words (i.e. words predefined to be too common to be of use for interpretation or queries, such as 'the', 'a', 'and', etc.)\n",
      "pp_uniq_non-stop_words  Proportion of non-stop words among unique words\n",
      "nb_links  Number of hyperlinks in the article\n",
      "nb_outside_links  Number of hyperlinks pointing to another website\n",
      "nb_images  Number of images in the article\n",
      "nb_videos  Number of videos in the article\n",
      "ave_word_length  Average word length\n",
      "nb_keywords  Number of keywords in the metadata\n",
      "category  Category of the article: 0-Lifestyle, 1-Entertainment, 2-Business, 3-Web, 4-Tech, 5-World\n",
      "nb_mina_mink  Minimum number of share counts among all articles with at least one keyword in common with the article\n",
      "nb_mina_maxk  Minimum number of maximum share counts per keyword\n",
      "nb_mina_avek  Minimum number of average share counts per keyword\n",
      "nb_maxa_mink  Maximum number of minimum share counts per keyword\n",
      "nb_maxa_maxk  Maximum number of share counts among all articles with at least one keyword in common with the article\n",
      "nb_maxa_avek  Maximum number of average share counts per keyword\n",
      "nb_avea_mink  Average number of minimum share counts per keyword\n",
      "nb_avea_maxk  Average number of maximum share counts per keyword\n",
      "nb_avea_avek  Average number of average share counts per keyword\n",
      "nb_min_linked  Minimum number of shares of articles from the same website linked within the article\n",
      "nb_max_linked  Maximum number of shares of articles from the same website linked within the article\n",
      "nb_ave_linked  Average number of shares of articles from the same website linked within the article\n",
      "weekday  Day of the week: 0-Monday, 1-Tuesday, 2-Wednesday, until 6-Sunday\n",
      "dist_topic_0  Distance to topic 0\n",
      "dist_topic_1  Distance to topic 1\n",
      "dist_topic_2  Distance to topic 2\n",
      "dist_topic_3  Distance to topic 3\n",
      "dist_topic_4  Distance to topic 4\n",
      "subj  Subjectivity\n",
      "polar  Sentiment polarity \n",
      "pp_pos_words  Proportion of positive words in the article\n",
      "pp_neg_words  Proportion of negative words in the article\n",
      "pp_pos_words_in_nonneutral  Proportion of positive words among the non-neutral words of the article\n",
      "ave_polar_pos  Average sentiment polarity of the positive words\n",
      "min_polar_pos  Minimum sentiment polarity of the positive words\n",
      "max_polar_pos  Maximum sentiment polarity of the positive words\n",
      "ave_polar_neg  Average sentiment polarity of the negative words\n",
      "min_polar_neg  Mimimum sentiment polarity of the negative words\n",
      "max_polar_neg  Maximum sentiment polarity of the negative words\n",
      "subj_title  Subjectivity of the title\n",
      "polar_title  Polarity of the title\n"
     ]
    }
   ],
   "source": [
    "# we display the description of the features\n",
    "!cat data/features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_names</th>\n",
       "      <th>feature_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb_words_title</td>\n",
       "      <td>Number of words in the article's titles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb_words_content</td>\n",
       "      <td>Number of words in the article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pp_uniq_words</td>\n",
       "      <td>Proportion of unique words in the article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pp_stop_words</td>\n",
       "      <td>Proportion of stop words (i.e. words predefine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pp_uniq_non-stop_words</td>\n",
       "      <td>Proportion of non-stop words among unique words</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature_names                                feature_description\n",
       "0          nb_words_title            Number of words in the article's titles\n",
       "1        nb_words_content                     Number of words in the article\n",
       "2           pp_uniq_words          Proportion of unique words in the article\n",
       "3           pp_stop_words  Proportion of stop words (i.e. words predefine...\n",
       "4  pp_uniq_non-stop_words    Proportion of non-stop words among unique words"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload of the features\n",
    "feature_data = pd.read_csv('data/features.txt', header=None, sep=\"  \", names=['feature_names', 'feature_description'])\n",
    "feature_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "** Now, let's load and look at the distribution of number of shares (output). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  Prediction\n",
       "0  2000         882\n",
       "1  2001        1102\n",
       "2  2002        1102\n",
       "3  2003        1001\n",
       "4  2004        1603"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data = pd.read_csv('data/train-targets.csv', sep=\",\")\n",
    "target_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the labels variying in <b>R</b>  we understang that is a regression problem. </n>\n",
    "Let's visualize the histogram of the label to see their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 10000.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_tr = target_data['Prediction'].values\n",
    "plt.hist(y_tr,bins=2000)\n",
    "plt.xlim((0,10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the distribution is somehow following a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "** Now, let's load and visualize the features. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_words_title</th>\n",
       "      <th>nb_words_content</th>\n",
       "      <th>pp_uniq_words</th>\n",
       "      <th>pp_stop_words</th>\n",
       "      <th>pp_uniq_non-stop_words</th>\n",
       "      <th>nb_links</th>\n",
       "      <th>nb_outside_links</th>\n",
       "      <th>nb_images</th>\n",
       "      <th>nb_videos</th>\n",
       "      <th>ave_word_length</th>\n",
       "      <th>...</th>\n",
       "      <th>pp_neg_words</th>\n",
       "      <th>pp_pos_words_in_nonneutral</th>\n",
       "      <th>ave_polar_pos</th>\n",
       "      <th>min_polar_pos</th>\n",
       "      <th>max_polar_pos</th>\n",
       "      <th>ave_polar_neg</th>\n",
       "      <th>min_polar_neg</th>\n",
       "      <th>max_polar_neg</th>\n",
       "      <th>subj_title</th>\n",
       "      <th>polar_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>9</td>\n",
       "      <td>843</td>\n",
       "      <td>0.5358</td>\n",
       "      <td>2.092000e-09</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.4437</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3160</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>9</td>\n",
       "      <td>805</td>\n",
       "      <td>0.4196</td>\n",
       "      <td>2.165000e-09</td>\n",
       "      <td>0.5693</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.3463</td>\n",
       "      <td>-0.7143</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>0.7594</td>\n",
       "      <td>1.163000e-08</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>12</td>\n",
       "      <td>201</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>9.259000e-09</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>13</td>\n",
       "      <td>673</td>\n",
       "      <td>0.4609</td>\n",
       "      <td>2.500000e-09</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.2435</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nb_words_title  nb_words_content  pp_uniq_words  pp_stop_words  \\\n",
       "2000               9               843         0.5358   2.092000e-09   \n",
       "2001               9               805         0.4196   2.165000e-09   \n",
       "2002               8               145         0.7594   1.163000e-08   \n",
       "2003              12               201         0.6359   9.259000e-09   \n",
       "2004              13               673         0.4609   2.500000e-09   \n",
       "\n",
       "      pp_uniq_non-stop_words  nb_links  nb_outside_links  nb_images  \\\n",
       "2000                  0.7469      15.0                 8         11   \n",
       "2001                  0.5693       8.0                 7          1   \n",
       "2002                  0.8488       7.0                 3          0   \n",
       "2003                  0.8148       7.0                 2          0   \n",
       "2004                  0.5950       8.0                 7          1   \n",
       "\n",
       "      nb_videos  ave_word_length  ...  pp_neg_words  \\\n",
       "2000          1                4  ...      0.019230   \n",
       "2001          0                4  ...      0.025710   \n",
       "2002          2                4  ...      0.007519   \n",
       "2003          0                4  ...      0.027030   \n",
       "2004          0                4  ...      0.021440   \n",
       "\n",
       "      pp_pos_words_in_nonneutral  ave_polar_pos  min_polar_pos  max_polar_pos  \\\n",
       "2000                      0.7143         0.4437        0.03333            1.0   \n",
       "2001                      0.5349         0.3081        0.05000            0.8   \n",
       "2002                      0.8333         0.3673        0.13640            0.5   \n",
       "2003                      0.7368         0.3721        0.13640            0.6   \n",
       "2004                      0.5625         0.3500        0.05000            0.6   \n",
       "\n",
       "      ave_polar_neg  min_polar_neg  max_polar_neg  subj_title  polar_title  \n",
       "2000        -0.3160        -0.8000          -0.05         0.0          0.0  \n",
       "2001        -0.3463        -0.7143          -0.10         0.9          0.3  \n",
       "2002        -0.2000        -0.2000          -0.20         0.0          0.0  \n",
       "2003        -0.4000        -0.4000          -0.40         0.0          0.0  \n",
       "2004        -0.2435        -0.8000          -0.10         0.0          0.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_feature_names = list(feature_data['feature_names'])\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv', header=None, sep=\" \", names=list_feature_names)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Here we load the testing data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_words_title</th>\n",
       "      <th>nb_words_content</th>\n",
       "      <th>pp_uniq_words</th>\n",
       "      <th>pp_stop_words</th>\n",
       "      <th>pp_uniq_non-stop_words</th>\n",
       "      <th>nb_links</th>\n",
       "      <th>nb_outside_links</th>\n",
       "      <th>nb_images</th>\n",
       "      <th>nb_videos</th>\n",
       "      <th>ave_word_length</th>\n",
       "      <th>...</th>\n",
       "      <th>pp_neg_words</th>\n",
       "      <th>pp_pos_words_in_nonneutral</th>\n",
       "      <th>ave_polar_pos</th>\n",
       "      <th>min_polar_pos</th>\n",
       "      <th>max_polar_pos</th>\n",
       "      <th>ave_polar_neg</th>\n",
       "      <th>min_polar_neg</th>\n",
       "      <th>max_polar_neg</th>\n",
       "      <th>subj_title</th>\n",
       "      <th>polar_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>258</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>6.897000e-09</td>\n",
       "      <td>0.6897</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01653</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.2967</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2344</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>263</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>6.623000e-09</td>\n",
       "      <td>0.8543</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04701</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2617</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>1281</td>\n",
       "      <td>0.4067</td>\n",
       "      <td>1.422000e-09</td>\n",
       "      <td>0.5903</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01512</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2403</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.0500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>107</td>\n",
       "      <td>0.8152</td>\n",
       "      <td>1.538000e-08</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02151</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.4881</td>\n",
       "      <td>0.28570</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   nb_words_title  nb_words_content  pp_uniq_words  pp_stop_words  \\\n",
       "0              12               258         0.5745   6.897000e-09   \n",
       "1               8                11         0.0000   1.000000e+00   \n",
       "2              10               263         0.7249   6.623000e-09   \n",
       "3              13              1281         0.4067   1.422000e-09   \n",
       "4               9               107         0.8152   1.538000e-08   \n",
       "\n",
       "   pp_uniq_non-stop_words  nb_links  nb_outside_links  nb_images  nb_videos  \\\n",
       "0                  0.6897       4.0                 2          0          0   \n",
       "1                  0.0000       0.0                 0          0          0   \n",
       "2                  0.8543       6.0                 3          2          0   \n",
       "3                  0.5903      29.0                 4          1          1   \n",
       "4                  0.8154       5.0                 2          0          0   \n",
       "\n",
       "   ave_word_length  ...  pp_neg_words  pp_pos_words_in_nonneutral  \\\n",
       "0                4  ...       0.01653                      0.7143   \n",
       "1                0  ...       0.00000                      0.0000   \n",
       "2                5  ...       0.04701                      0.5000   \n",
       "3                4  ...       0.01512                      0.7500   \n",
       "4                4  ...       0.02151                      0.6667   \n",
       "\n",
       "   ave_polar_pos  min_polar_pos  max_polar_pos  ave_polar_neg  min_polar_neg  \\\n",
       "0         0.2967        0.10000            1.0        -0.2344           -0.3   \n",
       "1         0.0000        0.00000            0.0         0.0000            0.0   \n",
       "2         0.2617        0.10000            1.0        -0.2170           -0.5   \n",
       "3         0.3585        0.03333            1.0        -0.2403           -0.5   \n",
       "4         0.4881        0.28570            1.0        -0.8000           -1.0   \n",
       "\n",
       "   max_polar_neg  subj_title  polar_title  \n",
       "0        -0.1875       0.125          0.0  \n",
       "1         0.0000       0.525          0.3  \n",
       "2        -0.1250       0.000         -0.2  \n",
       "3        -0.0500       0.000          0.0  \n",
       "4        -0.6000       0.000          0.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('data/test.csv', header=None, sep=\" \", names=list_feature_names)\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a bit how the fetures describe the overall informations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_words_title</th>\n",
       "      <th>nb_words_content</th>\n",
       "      <th>pp_uniq_words</th>\n",
       "      <th>pp_stop_words</th>\n",
       "      <th>pp_uniq_non-stop_words</th>\n",
       "      <th>nb_links</th>\n",
       "      <th>nb_outside_links</th>\n",
       "      <th>nb_images</th>\n",
       "      <th>nb_videos</th>\n",
       "      <th>ave_word_length</th>\n",
       "      <th>...</th>\n",
       "      <th>pp_neg_words</th>\n",
       "      <th>pp_pos_words_in_nonneutral</th>\n",
       "      <th>ave_polar_pos</th>\n",
       "      <th>min_polar_pos</th>\n",
       "      <th>max_polar_pos</th>\n",
       "      <th>ave_polar_neg</th>\n",
       "      <th>min_polar_neg</th>\n",
       "      <th>max_polar_neg</th>\n",
       "      <th>subj_title</th>\n",
       "      <th>polar_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.380000</td>\n",
       "      <td>562.619200</td>\n",
       "      <td>0.530260</td>\n",
       "      <td>3.020000e-02</td>\n",
       "      <td>0.672933</td>\n",
       "      <td>10.709400</td>\n",
       "      <td>7.42240</td>\n",
       "      <td>4.36960</td>\n",
       "      <td>1.234200</td>\n",
       "      <td>3.999000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016501</td>\n",
       "      <td>0.684178</td>\n",
       "      <td>0.354043</td>\n",
       "      <td>0.093984</td>\n",
       "      <td>0.757282</td>\n",
       "      <td>-0.254947</td>\n",
       "      <td>-0.514777</td>\n",
       "      <td>-0.104303</td>\n",
       "      <td>0.288572</td>\n",
       "      <td>0.071929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.104777</td>\n",
       "      <td>465.785259</td>\n",
       "      <td>0.136349</td>\n",
       "      <td>1.711544e-01</td>\n",
       "      <td>0.152879</td>\n",
       "      <td>10.700498</td>\n",
       "      <td>9.79755</td>\n",
       "      <td>7.95729</td>\n",
       "      <td>3.952781</td>\n",
       "      <td>0.785062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.190982</td>\n",
       "      <td>0.104455</td>\n",
       "      <td>0.071783</td>\n",
       "      <td>0.246997</td>\n",
       "      <td>0.124528</td>\n",
       "      <td>0.290311</td>\n",
       "      <td>0.089427</td>\n",
       "      <td>0.323541</td>\n",
       "      <td>0.260094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.430000e-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>0.472275</td>\n",
       "      <td>2.379500e-09</td>\n",
       "      <td>0.627575</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.307700</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.324525</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>4.082000e-09</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.714300</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.250100</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>724.250000</td>\n",
       "      <td>0.606725</td>\n",
       "      <td>6.667000e-09</td>\n",
       "      <td>0.754275</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021780</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.182400</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>7775.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>159.00000</td>\n",
       "      <td>128.00000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091040</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       nb_words_title  nb_words_content  pp_uniq_words  pp_stop_words  \\\n",
       "count     5000.000000       5000.000000    5000.000000   5.000000e+03   \n",
       "mean        10.380000        562.619200       0.530260   3.020000e-02   \n",
       "std          2.104777        465.785259       0.136349   1.711544e-01   \n",
       "min          4.000000          0.000000       0.000000   2.430000e-10   \n",
       "25%          9.000000        265.000000       0.472275   2.379500e-09   \n",
       "50%         10.000000        427.000000       0.539700   4.082000e-09   \n",
       "75%         12.000000        724.250000       0.606725   6.667000e-09   \n",
       "max         19.000000       7775.000000       1.000000   1.000000e+00   \n",
       "\n",
       "       pp_uniq_non-stop_words     nb_links  nb_outside_links   nb_images  \\\n",
       "count             5000.000000  5000.000000        5000.00000  5000.00000   \n",
       "mean                 0.672933    10.709400           7.42240     4.36960   \n",
       "std                  0.152879    10.700498           9.79755     7.95729   \n",
       "min                  0.000000     0.000000           0.00000     0.00000   \n",
       "25%                  0.627575     4.000000           1.00000     1.00000   \n",
       "50%                  0.691700     7.000000           4.00000     1.00000   \n",
       "75%                  0.754275    14.000000          10.00000     3.00000   \n",
       "max                  1.000000   162.000000         159.00000   128.00000   \n",
       "\n",
       "         nb_videos  ave_word_length  ...  pp_neg_words  \\\n",
       "count  5000.000000      5000.000000  ...   5000.000000   \n",
       "mean      1.234200         3.999000  ...      0.016501   \n",
       "std       3.952781         0.785062  ...      0.010663   \n",
       "min       0.000000         0.000000  ...      0.000000   \n",
       "25%       0.000000         4.000000  ...      0.009615   \n",
       "50%       0.000000         4.000000  ...      0.015115   \n",
       "75%       1.000000         4.000000  ...      0.021780   \n",
       "max      75.000000         7.000000  ...      0.091040   \n",
       "\n",
       "       pp_pos_words_in_nonneutral  ave_polar_pos  min_polar_pos  \\\n",
       "count                 5000.000000    5000.000000    5000.000000   \n",
       "mean                     0.684178       0.354043       0.093984   \n",
       "std                      0.190982       0.104455       0.071783   \n",
       "min                      0.000000       0.000000       0.000000   \n",
       "25%                      0.600000       0.307700       0.050000   \n",
       "50%                      0.714300       0.359000       0.100000   \n",
       "75%                      0.800000       0.411800       0.100000   \n",
       "max                      1.000000       0.950000       0.900000   \n",
       "\n",
       "       max_polar_pos  ave_polar_neg  min_polar_neg  max_polar_neg  \\\n",
       "count    5000.000000    5000.000000    5000.000000    5000.000000   \n",
       "mean        0.757282      -0.254947      -0.514777      -0.104303   \n",
       "std         0.246997       0.124528       0.290311       0.089427   \n",
       "min         0.000000      -1.000000      -1.000000      -1.000000   \n",
       "25%         0.600000      -0.324525      -0.700000      -0.125000   \n",
       "50%         0.800000      -0.250100      -0.500000      -0.100000   \n",
       "75%         1.000000      -0.182400      -0.300000      -0.050000   \n",
       "max         1.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "        subj_title  polar_title  \n",
       "count  5000.000000  5000.000000  \n",
       "mean      0.288572     0.071929  \n",
       "std       0.323541     0.260094  \n",
       "min       0.000000    -1.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.200000     0.000000  \n",
       "75%       0.500000     0.160500  \n",
       "max       1.000000     1.000000  \n",
       "\n",
       "[8 rows x 43 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this we can see that their is no missing informations, all the features have a count of 5000, but more importantly their is really important features such nb_words_content that describe a big variance (i.e hold a lot of informations) in comparison of the other features but it mean value it's also higher than the other. Hence we will need to standarized the values to avoid that this features bias the results since it will have a much higher weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "But before applying any transformation to the data. Let's explore the features and try to check if all of them are pertinent, the goal it's to remove the features we don't need and reduce the data dimensionality. This will give us better results and make the computation easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features we can remove are the correlated ones that have the lesser variance. for exemple if two features are correlated or nearly correlated which is more accurate here, we can remove one them since they are holding the same value. And to determine which one we can remove, we will compare their variance (i.e the informations they hold) and remove the one that have the lower one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by computing the correlation matrix of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corrMatrix = train_data.corr()\n",
    "sn.heatmap(corrMatrix, xticklabels=False, yticklabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap above illustrate the correlation between the feature. the brightest values (over >0.8) or the darkest (<-0.8) express the most correlated features. the diagonal hold only 1 values since each feature is fully correlated to itself.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's determine the most correlated features and compare their standard deviation. We will assume that the most correlated features have a absolute value higher than 0.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature_1               feature_2  correlation_value  std_feature_1  \\\n",
      "0  pp_uniq_words  pp_uniq_non-stop_words           0.937697       0.136335   \n",
      "1  pp_stop_words         ave_word_length          -0.898987       0.171137   \n",
      "2       nb_links        nb_outside_links           0.939545      10.699428   \n",
      "3   nb_mina_mink            nb_maxa_maxk          -0.860841      70.701093   \n",
      "4   nb_mina_maxk            nb_mina_avek           0.949055    2515.322786   \n",
      "5   nb_avea_maxk            nb_avea_avek           0.801864    4762.028495   \n",
      "6  nb_max_linked           nb_ave_linked           0.884761   34293.993901   \n",
      "\n",
      "   std_feature_2  \n",
      "0       0.152864  \n",
      "1       0.784983  \n",
      "2       9.796570  \n",
      "3  218895.020798  \n",
      "4     426.475670  \n",
      "5    1224.891423  \n",
      "6   21193.796884  \n"
     ]
    }
   ],
   "source": [
    "# return the indexes of the features that are highly correlated \n",
    "I,J=np.where((abs(corrMatrix) > 0.8) & (corrMatrix!=1))\n",
    "\n",
    "#remove the dupplicate\n",
    "for idx_I, elem_I in enumerate(I):\n",
    "    idx_J = np.where(J == elem_I)\n",
    "    idx_J = idx_J[0][0]\n",
    "    if(I[idx_J]==J[idx_I]):\n",
    "        J[idx_J]=0\n",
    "        I[idx_J]=0\n",
    "I=I[I!=0]\n",
    "J=J[J!=0]\n",
    "\n",
    "# get their names\n",
    "I_features_name=train_data.columns.values[I]\n",
    "J_features_name=train_data.columns.values[J]\n",
    "\n",
    "std_feature_I=[]\n",
    "std_feature_J=[]\n",
    "correlation_list=[]\n",
    "\n",
    "#compute their variance\n",
    "for idx, elem in enumerate(I_features_name):\n",
    "    correlation_list.append(corrMatrix.iloc[I[idx],J[idx]])\n",
    "    std_feature_I.append(np.std(train_data[elem]))\n",
    "    std_feature_J.append(np.std(train_data[J_features_name[idx]]))\n",
    "    \n",
    "#store them in a dictionary    \n",
    "data_correlation = {'feature_1': I_features_name,\n",
    "        'feature_2': J_features_name,\n",
    "        'correlation_value': correlation_list,\n",
    "        'std_feature_1': std_feature_I,\n",
    "        'std_feature_2': std_feature_J}\n",
    "#store them in a dataframe for the sake of the visualisation \n",
    "features_correlated_compar = pd.DataFrame(data_correlation, columns = ['feature_1','feature_2', 'correlation_value','std_feature_1', 'std_feature_2'])\n",
    " \n",
    "print(features_correlated_compar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the informations above we can determine the 6 features we can remove:\n",
    "<b>pp_uniq_words, pp_stop_words, nb_outside_links, nb_mina_mink, nb_mina_avek, nb_avea_avek, nb_ave_linked</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.drop(['pp_uniq_words', 'pp_stop_words', 'nb_outside_links', 'nb_mina_mink', 'nb_mina_avek', 'nb_avea_avek', 'nb_ave_linked'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the transformation of our features. We will work on our categorical features such as <b><i>weekday</i></b> and <b><i>category</i></b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize first our weekday feature histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.get([\"weekday\"]).plot.hist(grid=True, bins=20, rwidth=0.9,\n",
    "                   color='#607c8e')\n",
    "plt.title('weekday histogram')\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Commute Time')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for exemple we can clearly see that during weekend there is less publications than during the week. We can for exemple binarize this features between 'working days' and holidays. But here we will try to get more precision by encoding our weekdays into categorical data. Unfortunatly their will be a downside to this: increasing the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>nb_words_title</th>\n",
       "      <th>nb_words_content</th>\n",
       "      <th>pp_uniq_non-stop_words</th>\n",
       "      <th>nb_links</th>\n",
       "      <th>...</th>\n",
       "      <th>pp_neg_words</th>\n",
       "      <th>pp_pos_words_in_nonneutral</th>\n",
       "      <th>ave_polar_pos</th>\n",
       "      <th>min_polar_pos</th>\n",
       "      <th>max_polar_pos</th>\n",
       "      <th>ave_polar_neg</th>\n",
       "      <th>min_polar_neg</th>\n",
       "      <th>max_polar_neg</th>\n",
       "      <th>subj_title</th>\n",
       "      <th>polar_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>843</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.4437</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3160</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>805</td>\n",
       "      <td>0.5693</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.3463</td>\n",
       "      <td>-0.7143</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>201</td>\n",
       "      <td>0.8148</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>673</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.2435</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \\\n",
       "2000          1          0          0          0          0          0   \n",
       "2001          0          1          0          0          0          0   \n",
       "2002          0          0          0          0          0          0   \n",
       "2003          0          1          0          0          0          0   \n",
       "2004          0          1          0          0          0          0   \n",
       "\n",
       "      nb_words_title  nb_words_content  pp_uniq_non-stop_words  nb_links  ...  \\\n",
       "2000               9               843                  0.7469      15.0  ...   \n",
       "2001               9               805                  0.5693       8.0  ...   \n",
       "2002               8               145                  0.8488       7.0  ...   \n",
       "2003              12               201                  0.8148       7.0  ...   \n",
       "2004              13               673                  0.5950       8.0  ...   \n",
       "\n",
       "      pp_neg_words  pp_pos_words_in_nonneutral  ave_polar_pos  min_polar_pos  \\\n",
       "2000      0.019230                      0.7143         0.4437        0.03333   \n",
       "2001      0.025710                      0.5349         0.3081        0.05000   \n",
       "2002      0.007519                      0.8333         0.3673        0.13640   \n",
       "2003      0.027030                      0.7368         0.3721        0.13640   \n",
       "2004      0.021440                      0.5625         0.3500        0.05000   \n",
       "\n",
       "      max_polar_pos  ave_polar_neg  min_polar_neg  max_polar_neg  subj_title  \\\n",
       "2000            1.0        -0.3160        -0.8000          -0.05         0.0   \n",
       "2001            0.8        -0.3463        -0.7143          -0.10         0.9   \n",
       "2002            0.5        -0.2000        -0.2000          -0.20         0.0   \n",
       "2003            0.6        -0.4000        -0.4000          -0.40         0.0   \n",
       "2004            0.6        -0.2435        -0.8000          -0.10         0.0   \n",
       "\n",
       "      polar_title  \n",
       "2000          0.0  \n",
       "2001          0.3  \n",
       "2002          0.0  \n",
       "2003          0.0  \n",
       "2004          0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the category data and encode it using a dummy categorical encoding\n",
    "weekdays_data = pd.get_dummies(train_data['weekday'], prefix='weekday', drop_first=True)\n",
    "\n",
    "# Get the rest of the data\n",
    "other_data = train_data.drop(['weekday'], axis=1)\n",
    "\n",
    "# Create a new data set by concatenation of the new weekday data and the old rest of the data\n",
    "training_data = pd.concat([weekdays_data, other_data], axis=1)\n",
    "\n",
    "# Print the created training data.\n",
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B: We could encode our weekday feature in a periodical way as below. But this will not help us a lot.  \n",
    "As we see below the value are really spread between weekends and working days, there is no periodical relationship between the parameter for exemple between sunday and monday, so if we apply a periodical transformation to the data it will not help us.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_new = train_data.apply(lambda x: np.sin((2*math.pi*x)/7) if x.name == 'weekday' else x)\n",
    "\n",
    "train_data_new.get([\"weekday\"]).plot.hist(grid=True, bins=20, rwidth=0.9,\n",
    "                   color='#607c8e')\n",
    "plt.title('weekday histogram')\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Commute Time')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the same encoding to the 'category'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>...</th>\n",
       "      <th>pp_neg_words</th>\n",
       "      <th>pp_pos_words_in_nonneutral</th>\n",
       "      <th>ave_polar_pos</th>\n",
       "      <th>min_polar_pos</th>\n",
       "      <th>max_polar_pos</th>\n",
       "      <th>ave_polar_neg</th>\n",
       "      <th>min_polar_neg</th>\n",
       "      <th>max_polar_neg</th>\n",
       "      <th>subj_title</th>\n",
       "      <th>polar_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.4437</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3160</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.3463</td>\n",
       "      <td>-0.7143</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.3673</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.13640</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.2435</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category_1  category_2  category_3  category_4  category_5  weekday_1  \\\n",
       "2000           1           0           0           0           0          1   \n",
       "2001           0           0           0           0           1          0   \n",
       "2002           0           0           0           1           0          0   \n",
       "2003           0           0           0           1           0          0   \n",
       "2004           0           0           0           0           1          0   \n",
       "\n",
       "      weekday_2  weekday_3  weekday_4  weekday_5  ...  pp_neg_words  \\\n",
       "2000          0          0          0          0  ...      0.019230   \n",
       "2001          1          0          0          0  ...      0.025710   \n",
       "2002          0          0          0          0  ...      0.007519   \n",
       "2003          1          0          0          0  ...      0.027030   \n",
       "2004          1          0          0          0  ...      0.021440   \n",
       "\n",
       "      pp_pos_words_in_nonneutral  ave_polar_pos  min_polar_pos  max_polar_pos  \\\n",
       "2000                      0.7143         0.4437        0.03333            1.0   \n",
       "2001                      0.5349         0.3081        0.05000            0.8   \n",
       "2002                      0.8333         0.3673        0.13640            0.5   \n",
       "2003                      0.7368         0.3721        0.13640            0.6   \n",
       "2004                      0.5625         0.3500        0.05000            0.6   \n",
       "\n",
       "      ave_polar_neg  min_polar_neg  max_polar_neg  subj_title  polar_title  \n",
       "2000        -0.3160        -0.8000          -0.05         0.0          0.0  \n",
       "2001        -0.3463        -0.7143          -0.10         0.9          0.3  \n",
       "2002        -0.2000        -0.2000          -0.20         0.0          0.0  \n",
       "2003        -0.4000        -0.4000          -0.40         0.0          0.0  \n",
       "2004        -0.2435        -0.8000          -0.10         0.0          0.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the category data and encode it using a dummy categorical encoding\n",
    "category_data = pd.get_dummies(train_data['category'], prefix='category', drop_first=True)\n",
    "\n",
    "# Get the rest of the data\n",
    "other_data = training_data.drop(['category'], axis=1)\n",
    "\n",
    "# Create a new data set by concatenation of the new weekday data and the old rest of the data\n",
    "training_data = pd.concat([category_data, other_data], axis=1)\n",
    "\n",
    "# Print the created training data.\n",
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not forget to apply the same transformation to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>...</th>\n",
       "      <th>pp_neg_words</th>\n",
       "      <th>pp_pos_words_in_nonneutral</th>\n",
       "      <th>ave_polar_pos</th>\n",
       "      <th>min_polar_pos</th>\n",
       "      <th>max_polar_pos</th>\n",
       "      <th>ave_polar_neg</th>\n",
       "      <th>min_polar_neg</th>\n",
       "      <th>max_polar_neg</th>\n",
       "      <th>subj_title</th>\n",
       "      <th>polar_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01653</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.2967</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2344</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04701</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2617</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01512</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2403</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.0500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02151</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.4881</td>\n",
       "      <td>0.28570</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_1  category_2  category_3  category_4  category_5  weekday_1  \\\n",
       "0           0           0           0           0           1          0   \n",
       "1           0           0           0           0           1          0   \n",
       "2           0           0           0           0           1          0   \n",
       "3           0           0           0           1           0          0   \n",
       "4           0           0           0           0           1          0   \n",
       "\n",
       "   weekday_2  weekday_3  weekday_4  weekday_5  ...  pp_neg_words  \\\n",
       "0          0          0          0          0  ...       0.01653   \n",
       "1          1          0          0          0  ...       0.00000   \n",
       "2          0          1          0          0  ...       0.04701   \n",
       "3          0          0          0          0  ...       0.01512   \n",
       "4          1          0          0          0  ...       0.02151   \n",
       "\n",
       "   pp_pos_words_in_nonneutral  ave_polar_pos  min_polar_pos  max_polar_pos  \\\n",
       "0                      0.7143         0.2967        0.10000            1.0   \n",
       "1                      0.0000         0.0000        0.00000            0.0   \n",
       "2                      0.5000         0.2617        0.10000            1.0   \n",
       "3                      0.7500         0.3585        0.03333            1.0   \n",
       "4                      0.6667         0.4881        0.28570            1.0   \n",
       "\n",
       "   ave_polar_neg  min_polar_neg  max_polar_neg  subj_title  polar_title  \n",
       "0        -0.2344           -0.3        -0.1875       0.125          0.0  \n",
       "1         0.0000            0.0         0.0000       0.525          0.3  \n",
       "2        -0.2170           -0.5        -0.1250       0.000         -0.2  \n",
       "3        -0.2403           -0.5        -0.0500       0.000          0.0  \n",
       "4        -0.8000           -1.0        -0.6000       0.000          0.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the unsusefull features\n",
    "test_data=test_data.drop(['pp_uniq_words', 'pp_stop_words', 'nb_outside_links', 'nb_mina_mink', 'nb_mina_avek', 'nb_avea_avek', 'nb_ave_linked'], axis = 1)\n",
    "\n",
    "# Get the category data and encode it using a dummy categorical encoding\n",
    "weekdays_data = pd.get_dummies(test_data['weekday'], prefix='weekday', drop_first=True)\n",
    "\n",
    "# Get the rest of the data\n",
    "other_data = test_data.drop(['weekday'], axis=1)\n",
    "\n",
    "# Create a new data set by concatenation of the new weekday data and the old rest of the data\n",
    "test_data = pd.concat([weekdays_data, other_data], axis=1)\n",
    "\n",
    "# Get the category data and encode it using a dummy categorical encoding\n",
    "category_data = pd.get_dummies(test_data['category'], prefix='category', drop_first=True)\n",
    "\n",
    "# Get the rest of the data\n",
    "other_data = test_data.drop(['category'], axis=1)\n",
    "\n",
    "# Create a new data set by concatenation of the new weekday data and the old rest of the data\n",
    "test_data = pd.concat([category_data, other_data], axis=1)\n",
    "\n",
    "# Print the created test data.\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In our regression problem here, we have many features expressed as continuous independent variables evolving in different scales, standardizing our data might be usefull. If we do not their contributions to the model may not be equal, since the features with higher values range will have a much important weight and this may biased our model. Especially if we use distance based models such as the ridge regression that we will use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standardizer object and fit it to the training data.\n",
    "std_scale = preprocessing.StandardScaler().fit(training_data)\n",
    "\n",
    "# Apply the standardization to the training and the test data.\n",
    "training_data_std = std_scale.transform(training_data)\n",
    "test_data_std = std_scale.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the standarization take in account only the training data, with that we avoid leaking of a small amount of information from test to train and hence biasing our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " In this section, we will start our model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try two different but simple linear model whitout tuning their hyperparameters just to have a quick overview on their behavior. A logistic regression with and without a ridge regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's start with the <b>non regularized logistic regression</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up first a <b>Kfold</b> to cross validate our model. It will help un to better evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up folds for cross_validation\n",
    "folds_regr = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used a Stratified KFold so that the different folds have the same proportion of the different sample. Hence we will have a better overall estimation of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's initialize our model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a LogisticRegression model. saga:5min 0.8 default:10min 0.9, lbfgs: 30sec 0.44\n",
    "logreg = linear_model.LogisticRegression(random_state=0, penalty=\"none\", solver=\"lbfgs\")\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(logreg, training_data_std, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I tried different solver \"saga\", \"sag\", \"lbfgs\", \"liblinear\" none of them were converging and their computational time was high (more than 5min per fold). The only one that was ok is \"lfbgs\" (30sec per fold) but as said before is not converging.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1220226166895477\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_logreg=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cross validate the <b>l2 regularized logistic regression model</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a LogisticRegression model. saga:5min 0.8 default:10min 0.9, lbfgs: 30sec 0.44\n",
    "logreg_l2 = linear_model.LogisticRegression(random_state=0, penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(logreg_l2, training_data_std, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0862326656072412\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_logreg_l2=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_logreg_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as shown above their two score are quite similar. To have a better model we will try to thune the hyperparameters of our ridge regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that we will use a <b>grid Search cross validation</b> to try different combination of hyperparameters and evaluate their performane. Then we will select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select the best hyperparam <b>'C'</b> (i.e the regularization coef) for the <b>ridge logistic regression</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-05 2.06913808e-05 4.28133240e-05 8.85866790e-05\n",
      " 1.83298071e-04 3.79269019e-04 7.84759970e-04 1.62377674e-03\n",
      " 3.35981829e-03 6.95192796e-03 1.43844989e-02 2.97635144e-02\n",
      " 6.15848211e-02 1.27427499e-01 2.63665090e-01 5.45559478e-01\n",
      " 1.12883789e+00 2.33572147e+00 4.83293024e+00 1.00000000e+01]\n"
     ]
    }
   ],
   "source": [
    "# Create a range of values to test for the parameter C\n",
    "cvalues_list = np.logspace(-5, 1, 20)\n",
    "print(cvalues_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=Fals...\n",
       "       1.83298071e-04, 3.79269019e-04, 7.84759970e-04, 1.62377674e-03,\n",
       "       3.35981829e-03, 6.95192796e-03, 1.43844989e-02, 2.97635144e-02,\n",
       "       6.15848211e-02, 1.27427499e-01, 2.63665090e-01, 5.45559478e-01,\n",
       "       1.12883789e+00, 2.33572147e+00, 4.83293024e+00, 1.00000000e+01])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize cvalue\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', solver=\"lbfgs\")\n",
    "param_grid = {'C': cvalues_list\n",
    "             }\n",
    "reg_l2_opt = GridSearchCV(classifier, \n",
    "                                param_grid, \n",
    "                                cv=folds_regr,\n",
    "                                scoring=\"neg_mean_squared_log_error\",\n",
    "                                n_jobs=-1)     \n",
    "reg_l2_opt.fit(training_data_std, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score  -0.9317381705802109\n",
      "Scaled, l2 regularization (C=6.16e-02): rmsle = 0.965\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=reg_l2_opt.best_score_\n",
    "print(\"best_score \", best_score)\n",
    "# optimal value of C\n",
    "cvalue_opt = dict(reg_l2_opt.best_estimator_.get_params(deep=False).items())['C']\n",
    "rmsle_logreg_l2_opt=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled, l2 regularization (C=%.2e): rmsle = %.3f\" % (cvalue_opt, \n",
    "                                                               rmsle_logreg_l2_opt))\n",
    "\n",
    "logreg_l2_opt=reg_l2_opt.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the score is slightly better now, but let's see if we can find a better model. For this we will try some non parametric model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. non parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let's try the <b>SVM</b> (support vector Machine) with a <b>Radial Basis Function</b> (RBF) kernel. Since it's a regression problem we will use the fonction SVR(support vector regression) instead of SVC(support vector classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just try the model with random or default hyperparameters to see how it behave. We will thune them later with a search grid cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "svr=SVR(C=1.0, kernel=\"rbf\", gamma=1e-2)\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(svr, training_data_std, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9356206387182059\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_svr=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the score above, even if the hyperparameters are choosen randomly, the score is better that the ridge regression which is a good indicator. Let's see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choisir 6 valeurs pour C, entre 1e-2 et 1e3\n",
    "C_range = np.logspace(-2, 3, 6)\n",
    "\n",
    "# choisir 4 valeurs pour gamma, entre 1e-2 et 10\n",
    "gamma_range = np.logspace(-2, 1, 4)\n",
    "\n",
    "# choisir 4 valeurs pour epsilon, entre 1e-2 et 10\n",
    "epsilon_range = np.logspace(-2, 1, 4)\n",
    "\n",
    "# grille de paramètres\n",
    "parameters = {'C': C_range, 'gamma': gamma_range, 'epsilon':epsilon_range}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,\n",
       "                           epsilon=0.1, gamma='auto_deprecated', kernel='rbf',\n",
       "                           max_iter=-1, shrinking=True, tol=0.001,\n",
       "                           verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'epsilon': array([ 0.01,  0.1 ,  1.  , 10.  ]),\n",
       "                         'gamma': array([ 0.01,  0.1 ,  1.  , 10.  ])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a model\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "\n",
    "# initialize cross validation\n",
    "clf = GridSearchCV(estimator=svr_rbf, \n",
    "                   param_grid=parameters,\n",
    "                   cv=folds_regr,\n",
    "                   scoring=\"neg_mean_squared_log_error\",\n",
    "                   n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "clf.fit(training_data_std, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score  -0.7760634449673002\n",
      "Scaled, SVM rbf kernel (C=1.00e+03), (gamma=0.010), (epsilon=10.000): rmsle = 0.881\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=clf.best_score_\n",
    "print(\"best_score \", best_score)\n",
    "# optimal value of C\n",
    "cvalue_opt = dict(clf.best_estimator_.get_params(deep=False).items())['C']\n",
    "gamma_opt = dict(clf.best_estimator_.get_params(deep=False).items())['gamma']\n",
    "epsilon_opt = dict(clf.best_estimator_.get_params(deep=False).items())['epsilon']\n",
    "rmsle_svr_rbf_opt=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled, SVM rbf kernel (C=%.2e), (gamma=%.3f), (epsilon=%.3f): rmsle = %.3f\" % (cvalue_opt, \n",
    "                                                gamma_opt,\n",
    "                                                epsilon_opt,\n",
    "                                                rmsle_svr_rbf_opt))\n",
    "regr_svr_rbf_opt=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well as shown above the grid search allow us to select better hyperparameters for the model, we even reach a higher score than before. good news! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the same <b>SVM</b> but with a <b>linear kernel</b>. Let's try it first with some default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#initialize a model\n",
    "svr=SVR(C=1.0, kernel=\"linear\")\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(svr, training_data_std, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9017672978249661\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_svr_lin=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_svr_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is ok. Let's try to improve it by tuning <b>C</b> and <b>epsilon</b> with a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choisir 6 valeurs pour C, entre 1e-2 et 1e3\n",
    "C_range = np.logspace(-2, 3, 6)\n",
    "\n",
    "# choisir 4 valeurs pour epsilon, entre 1e-2 et 10\n",
    "epsilon_range = np.logspace(-3, 1, 5)\n",
    "# grille de paramètres\n",
    "parameters = {'C': C_range, 'epsilon':epsilon_range}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,\n",
       "                           epsilon=0.1, gamma='auto_deprecated',\n",
       "                           kernel='linear', max_iter=-1, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'epsilon': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a model\n",
    "svr_lin = SVR(kernel='linear')\n",
    "\n",
    "# initialize cross validation\n",
    "clf = GridSearchCV(estimator=svr_lin, \n",
    "                   param_grid=parameters,\n",
    "                   cv=folds_regr,\n",
    "                   scoring=\"neg_mean_squared_log_error\",\n",
    "                    n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "clf.fit(training_data_std, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled, SVM linear (C=1000.00), (epsilon=10.000) rmsle=0.8953\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=clf.best_score_\n",
    "\n",
    "# optimal value of C\n",
    "c_value_opt = dict(clf.best_estimator_.get_params(deep=False).items())['C']\n",
    "epsilon_opt=dict(clf.best_estimator_.get_params(deep=False).items())['epsilon']\n",
    "\n",
    "rmsle_svr_lin_opt=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled, SVM linear (C=%.2f), (epsilon=%.3f) rmsle=%.4f\" % (c_value_opt, \n",
    "                                                epsilon_opt,\n",
    "                                                rmsle_svr_lin_opt))\n",
    "regr_svr_lin_opt=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is slightly different from the SVM with an <b>exponential kernel (rbf)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now a <b>random forest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# initialize a model\n",
    "rf=RandomForestRegressor(n_jobs=-1, n_estimators=80, max_features=0.5, min_samples_leaf=30 )\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(rf, training_data_std, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0185782696476255\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_rf=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunatly here the results are lower than expected. Let's try if we can do better by tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are primarily 3 features which can be tuned to improve the predictive power of the model :\n",
    "\n",
    "<b>1.a. max_features:</b>\n",
    "These are the maximum number of features Random Forest is allowed to try in individual tree. Increasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered. However, this is not necessarily true as this decreases the diversity of individual tree which is the USP of random forest. But, for sure, we decrease the speed of the algorithm by increasing the max_features. \n",
    "\n",
    "<b>1.b. n_estimators :</b>\n",
    "This is the number of trees we want to build before taking the maximum voting or averages of predictions. Higher number of trees gives us better performance but makes our code slower. \n",
    " \n",
    "<b>1.c. min_sample_leaf :</b>\n",
    "specifies the minimum number of samples required to be at a leaf node.\n",
    "A smaller leaf makes the model more prone to capturing noise in train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators='warn', n_jobs=-1,\n",
       "                                             oob_score=False, random_state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_features': [0.8], 'min_samples_leaf': [30],\n",
       "                         'n_estimators': [800]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees to use\n",
    "list_n_trees = [500, 800, 1000, 1500]\n",
    "max_features = [0.2, 0.3, 0.5, 0.7, 0.8]\n",
    "list_min_sample_leaf=[10, 30, 50, 70]\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "param_grid ={'n_estimators': list_n_trees, 'max_features':max_features, 'min_samples_leaf':list_min_sample_leaf }\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a random forest with these parameters.\n",
    "regr_rf = GridSearchCV(RandomForestRegressor(n_jobs=-1), \n",
    "                                    param_grid=param_grid,\n",
    "                                    scoring='neg_mean_squared_log_error',\n",
    "                                    cv=folds_regr,\n",
    "                                    n_jobs=-1)\n",
    "# run the cross validation using train dataset\n",
    "regr_rf.fit(training_data_std, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score  -1.0334853779084037\n",
      "Scaled, RF (nb_trees=8.00e+02), (max_features=0.800), (min_sample_leaf=30.000): rmsle = 1.017\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=regr_rf.best_score_\n",
    "print(\"best_score \", best_score)\n",
    "# optimal value of C\n",
    "n_trees_opt = dict(regr_rf.best_estimator_.get_params(deep=False).items())['n_estimators']\n",
    "max_features_opt = dict(regr_rf.best_estimator_.get_params(deep=False).items())['max_features']\n",
    "min_sample_leaf_opt = dict(regr_rf.best_estimator_.get_params(deep=False).items())['min_samples_leaf']\n",
    "rmsle_rf_opt=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled, RF (nb_trees=%.2e), (max_features=%.3f), (min_sample_leaf=%.3f): rmsle = %.3f\" % (n_trees_opt, \n",
    "                                                max_features_opt,\n",
    "                                                min_sample_leaf_opt,\n",
    "                                                rmsle_rf_opt))\n",
    "regr_rf_opt=rf_best=regr_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to say that I was expeting better result since the litterature says that this model work quite well with regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another approach  <b>Bagging trees</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's explore a simple <b>Bagging trees model</b> using a simple <b>SVM as a base estimator</b>. We will set up the hyperparameters later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "regr_br = BaggingRegressor(base_estimator=SVR(kernel='rbf'), n_estimators=300, max_features=0.3, max_samples=0.2, n_jobs=-1, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(regr_br, training_data_std, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9358820042623099\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_br=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, the result is better than the random forest. let's see if we can to do better. Here the hyper parameters we can tune are:\n",
    "\n",
    "<b>1.a. base estimator:</b>\n",
    "the basic model used to train each tree.\n",
    "\n",
    "<b>1.b. n_estimators :</b>\n",
    "This is the number of trees we want to build before taking the maximum voting or averages of predictions. Higher number of trees gives us better performance but makes our code slower. \n",
    "\n",
    "<b>1.c. max_features :</b>\n",
    "amount of features used to train each base estimator. \n",
    " \n",
    "<b>1.d. max_samples :</b>\n",
    "amount of samples used to train each base estimator. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=BaggingRegressor(base_estimator=SVR(C=1.0,\n",
       "                                                           cache_size=200,\n",
       "                                                           coef0=0.0, degree=3,\n",
       "                                                           epsilon=0.1,\n",
       "                                                           gamma='auto_deprecated',\n",
       "                                                           kernel='rbf',\n",
       "                                                           max_iter=-1,\n",
       "                                                           shrinking=True,\n",
       "                                                           tol=0.001,\n",
       "                                                           verbose=False),\n",
       "                                        bootstrap=True,\n",
       "                                        bootstrap_features=False,\n",
       "                                        max_features=1.0, max_samples=1.0,\n",
       "                                        n_estimators=10, n_jobs=-1,\n",
       "                                        oob_score=False, random_state=0,\n",
       "                                        verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_features': [0.5], 'max_samples': [0.8],\n",
       "                         'n_estimators': [30]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees to use\n",
    "list_n_trees = [30, 40, 50, 60, 70] \n",
    "max_features = [0.1, 0.3, 0.5, 0.7]\n",
    "max_samples=[0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "param_grid ={'n_estimators': list_n_trees, 'max_features':max_features, 'max_samples':max_samples}\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a bagging trees with these parameters.\n",
    "regr_br = GridSearchCV(BaggingRegressor(SVR(kernel='rbf'), n_jobs=-1, random_state=0), \n",
    "                                    param_grid=param_grid,\n",
    "                                    scoring='neg_mean_squared_log_error',\n",
    "                                    cv=folds_regr,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "regr_br.fit(training_data_std, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled, BR (nb_trees=3.00e+01), (max_features=0.500), (max_samples=0.800), (base_estimator=SVR): rmsle = 0.933\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=regr_br.best_score_\n",
    "\n",
    "# optimal value of C\n",
    "n_trees_opt = dict(regr_br.best_estimator_.get_params(deep=False).items())['n_estimators']\n",
    "max_features_opt = dict(regr_br.best_estimator_.get_params(deep=False).items())['max_features']\n",
    "max_samples_opt = dict(regr_br.best_estimator_.get_params(deep=False).items())['max_samples']\n",
    "base_estimator_opt=dict(regr_br.best_estimator_.get_params(deep=False).items())['base_estimator']\n",
    "\n",
    "rmsle_br_opt=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled, BR (nb_trees=%.2e), (max_features=%.3f), (max_samples=%.3f), (base_estimator=%.3s): rmsle = %.3f\" % (n_trees_opt, \n",
    "                                                max_features_opt,\n",
    "                                                max_samples_opt,\n",
    "                                                base_estimator_opt,                                                                    \n",
    "                                                rmsle_br_opt))\n",
    "regr_br_opt=regr_br.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to say that I was surprised when I got better results than the random forest with the bagging trees. Probably it's due to the use of an SVM as a base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found an interesting model on the literatture called <b>Multivariate Adaptive Regression Splines</b> also known as <b>MARS</b> an approach that seems adapted to regression problems. MARS model, implemented by the Earth class, is a flexible regression method that automatically searches for interactions and non-linear relationships using hinge functions. Let's try it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not implement in scilit learn, the first step is to install the py-earth library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn-contrib-py-earth in /home/jupyter-nezar.lheimeur/.local/lib/python3.6/site-packages (0.1.0)\n",
      "Requirement already satisfied: six in /opt/tljh/user/lib/python3.6/site-packages (from sklearn-contrib-py-earth) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /opt/tljh/user/lib/python3.6/site-packages (from sklearn-contrib-py-earth) (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.16 in /opt/tljh/user/lib/python3.6/site-packages (from sklearn-contrib-py-earth) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/tljh/user/lib/python3.6/site-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/tljh/user/lib/python3.6/site-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn-contrib-py-earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can load the library and print the version in a Python script to confirm it was installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "# check pyearth version\n",
    "import pyearth\n",
    "# display version\n",
    "print(pyearth.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MARS model can be created with default model hyperparameters by creating an instance of the Earth class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyearth import Earth\n",
    "# define the model\n",
    "model_mars = Earth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly here this model give us <b>negative values for the prediction</b> so we cannot cross validate it using <b>the mean squared log error</b> as an evaluation metric since the log function only accept positive values. So we need to to use the absolute values of the prediction to compute the rmsle. To do so we need to generate a new scoring function as shown bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rmsle_error(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true, abs(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#evaluation of the model\n",
    "score = cross_val_score(model_mars, training_data_std, y_tr, scoring=make_scorer(custom_rmsle_error, greater_is_better=False), cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0546180920504296\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_mars=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_mars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to tune it's hyperparameters using a gridSearch but the computational time explose when we try a higher degree than 1. after 30min of waiting to generate a model with max_term=100 and max_degree=2 I got lower result than the simpliest model max_term=50 and max_degree=1. Lack of time and patience I assumed that the best parameters here are for max_degree=1 and max_term=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For last i'll try the <b>gradient Boosting algorithm</b>. Since it also generate negative predective values we will use the customise scoring function we used for <b>MARS</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "regr_gb = GradientBoostingRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# compute the score \n",
    "score = cross_val_score(regr_gb, training_data_std, y_tr, scoring=make_scorer(custom_rmsle_error, greater_is_better=False), cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.043556608256238\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_gb=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We runned first the the alfgorithm with the default value and we got a bad score. let's see if we can do better with the hyperparameters tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='auto', random_state=0,\n",
       "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                          verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_gb.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingRegressor(alpha=0.9,\n",
       "                                                 criterion='friedman_mse',\n",
       "                                                 init=None, learning_rate=0.1,\n",
       "                                                 loss='ls', max_depth=3,\n",
       "                                                 max_features=None,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_spl...\n",
       "                                                 verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.15], 'max_depth': [2],\n",
       "                         'max_features': [0.1], 'min_samples_leaf': [5],\n",
       "                         'min_weight_fraction_leaf': [0], 'n_estimators': [700],\n",
       "                         'subsample': [0.001]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(custom_rmsle_error, greater_is_better=False),\n",
       "             verbose=0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees to use\n",
    "list_n_trees = [100, 300, 500, 700, 900, 1000] \n",
    "max_features =[0.1, 0.3, 0.5, 0.7]\n",
    "min_samples_leaf=[1, 2, 5, 10]\n",
    "max_depth=[2, 3, 4]\n",
    "subsample= [0.001, 0.01, 0.1]\n",
    "min_weight_fraction_leaf=[0, 0.1, 0.15, 0.2]\n",
    "learning_rate=[ 0.01, 0.05 ,0.1, 0.15]\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "param_grid ={'n_estimators': list_n_trees, 'max_features':max_features, 'min_samples_leaf':min_samples_leaf, 'max_depth':max_depth, 'subsample': subsample, 'min_weight_fraction_leaf': min_weight_fraction_leaf, 'learning_rate': learning_rate}\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a bagging trees with these parameters.\n",
    "regr_gb = GridSearchCV(GradientBoostingRegressor(random_state=0), \n",
    "                                    param_grid=param_grid,\n",
    "                                    scoring=make_scorer(custom_rmsle_error, greater_is_better=False),\n",
    "                                    cv=folds_regr,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "regr_gb.fit(training_data_std, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled, GB (nb_trees=7.00e+02), (max_features=0.100), (min_samples_leaf_opt=5.000), (max_depth_opt=2.000), (subsample_opt=0.001000), (min_weight_fraction_leaf=0.000000), (learning_rate=0.150) : rmsle = 1.251\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=regr_gb.best_score_\n",
    "\n",
    "# optimal value of C\n",
    "n_trees_opt = dict(regr_gb.best_estimator_.get_params(deep=False).items())['n_estimators']\n",
    "max_features_opt = dict(regr_gb.best_estimator_.get_params(deep=False).items())['max_features']\n",
    "min_samples_leaf_opt = dict(regr_gb.best_estimator_.get_params(deep=False).items())['min_samples_leaf']\n",
    "max_depth_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['max_depth']\n",
    "subsample_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['subsample']\n",
    "min_weight_fraction_leaf_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['min_weight_fraction_leaf']\n",
    "learning_rate_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['learning_rate']\n",
    "\n",
    "\n",
    "\n",
    "rmsle_gb_opt=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled, GB (nb_trees=%.2e), (max_features=%.3f), (min_samples_leaf_opt=%.3f), (max_depth_opt=%.3f), (subsample_opt=%f), (min_weight_fraction_leaf=%f), (learning_rate=%.3f) : rmsle = %.3f\" % \n",
    "                                                (n_trees_opt, \n",
    "                                                max_features_opt,\n",
    "                                                min_samples_leaf_opt,\n",
    "                                                max_depth_opt,\n",
    "                                                subsample_opt,\n",
    "                                                min_weight_fraction_leaf_opt,\n",
    "                                                learning_rate_opt,\n",
    "                                                rmsle_gb_opt))\n",
    "regr_gb_opt=regr_gb.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We' ve got a slightly better result but nothing special. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's resume our result in a dataframe shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>RMSLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge logistic reg</td>\n",
       "      <td>{'C': 0.06158482110660261}</td>\n",
       "      <td>0.965266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM-rbf</td>\n",
       "      <td>{'C': 1000.0, 'gamma': 0.01, 'epsilon': 10.0}</td>\n",
       "      <td>0.880945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM-linear</td>\n",
       "      <td>{'C': 1000.0, 'epsilon': 10.0}</td>\n",
       "      <td>0.895285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_features': 0.8, 'n_estimators': 800, 'mi...</td>\n",
       "      <td>1.016605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>{'base_estimator': SVR(C=1.0, cache_size=200, ...</td>\n",
       "      <td>0.933091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MARS</td>\n",
       "      <td>{'max_degree': [None, 'default: 1'], 'max_term...</td>\n",
       "      <td>1.054618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>{'n_estimators': 700, 'max_features': 0.1, 'mi...</td>\n",
       "      <td>1.250970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model                                         Parameters  \\\n",
       "0  Ridge logistic reg                         {'C': 0.06158482110660261}   \n",
       "1             SVM-rbf      {'C': 1000.0, 'gamma': 0.01, 'epsilon': 10.0}   \n",
       "2          SVM-linear                     {'C': 1000.0, 'epsilon': 10.0}   \n",
       "3       Random Forest  {'max_features': 0.8, 'n_estimators': 800, 'mi...   \n",
       "4             Bagging  {'base_estimator': SVR(C=1.0, cache_size=200, ...   \n",
       "5                MARS  {'max_degree': [None, 'default: 1'], 'max_term...   \n",
       "6   Gradient Boosting  {'n_estimators': 700, 'max_features': 0.1, 'mi...   \n",
       "\n",
       "      RMSLE  \n",
       "0  0.965266  \n",
       "1  0.880945  \n",
       "2  0.895285  \n",
       "3  1.016605  \n",
       "4  0.933091  \n",
       "5  1.054618  \n",
       "6  1.250970  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['Ridge logistic reg',{'C': logreg_l2_opt.get_params(deep=False)['C']}, rmsle_logreg_l2_opt],\n",
    "        ['SVM-rbf',{'C': regr_svr_rbf_opt.get_params(deep=False)['C'], 'gamma': regr_svr_rbf_opt.get_params(deep=False)['gamma'], 'epsilon': regr_svr_rbf_opt.get_params(deep=False)['epsilon'] }, rmsle_svr_rbf_opt],\n",
    "        ['SVM-linear',{'C': regr_svr_lin_opt.get_params(deep=False)['C'], 'epsilon': regr_svr_lin_opt.get_params(deep=False)['epsilon']}, rmsle_svr_lin_opt],\n",
    "        ['Random Forest',{'max_features': regr_rf_opt.get_params(deep=False)['max_features'], 'n_estimators': regr_rf_opt.get_params(deep=False)['n_estimators'], 'min_samples_leaf': regr_rf_opt.get_params(deep=False)['min_samples_leaf'] }, rmsle_rf_opt],\n",
    "        ['Bagging',{'base_estimator': regr_br_opt.get_params(deep=False)['base_estimator'], 'max_features': regr_br_opt.get_params(deep=False)['max_features'], 'n_estimators': regr_br_opt.get_params(deep=False)['n_estimators'], 'max_samples': regr_br_opt.get_params(deep=False)['max_samples'] }, rmsle_br_opt],\n",
    "        ['MARS', {'max_degree': [model_mars.get_params(deep=False)['max_degree'] ,\"default: 1\"] , 'max_terms': [model_mars.get_params(deep=False)['max_terms'], \"default: min(2 * nb_features + nb_rows // 10, 400)\"]}, rmsle_mars ],\n",
    "        ['Gradient Boosting',{'n_estimators': regr_gb_opt.get_params(deep=False)['n_estimators'], 'max_features': regr_gb_opt.get_params(deep=False)['max_features'], 'min_samples_leaf': regr_gb_opt.get_params(deep=False)['min_samples_leaf'],  'max_depth': regr_gb_opt.get_params(deep=False)['max_depth'], 'subsample': regr_gb_opt.get_params(deep=False)['subsample'], 'min_weight_fraction_leaf': regr_gb_opt.get_params(deep=False)['min_weight_fraction_leaf'], 'learning_rate': regr_gb_opt.get_params(deep=False)['learning_rate']}, rmsle_gb_opt]]\n",
    "df = pd.DataFrame(data,columns=['model','Parameters', 'RMSLE'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will apply a <b>PCA</b> to our data to reduce their dimensionality and rerun to most accurate of our above models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "#compute our PCA and fit it to the scaled data\n",
    "pca = decomposition.PCA(n_components=0.95)\n",
    "pca.fit(training_data_std)\n",
    "\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected the components that explain <b>95% of the variance</b> (i.e informations). let's visualize how much variance each of them hold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Fraction of variance explained')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(32), pca.explained_variance_ratio_)\n",
    "plt.xlim([-1, 32])\n",
    "plt.xlabel(\"Number of PCs\", fontsize=16)\n",
    "plt.ylabel(\"Fraction of variance explained\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's project our data in these Principal components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_projected = pca.transform(training_data_std)\n",
    "X_test_projected = pca.transform(test_data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train again our best previous model on our reduced dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's start with the <b>ridge regressor</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a LogisticRegression model. saga:5min 0.8 default:10min 0.9, lbfgs: 30sec 0.44\n",
    "logreg_l2_pca = linear_model.LogisticRegression(random_state=0, penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(logreg_l2_pca, X_train_projected, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1121792724393915\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_logreg_l2_pca=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_logreg_l2_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as shown above their two score are quite similar. To have a better model we will try to thune the hyperparameters of our ridge regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that we will use a <b>grid Search cross validation</b> to try different combination of hyperparameters and evaluate their performane. Then we will select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select the best hyperparam <b>'C'</b> (i.e the regularization coef) for the <b>ridge logistic regression</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-05 2.06913808e-05 4.28133240e-05 8.85866790e-05\n",
      " 1.83298071e-04 3.79269019e-04 7.84759970e-04 1.62377674e-03\n",
      " 3.35981829e-03 6.95192796e-03 1.43844989e-02 2.97635144e-02\n",
      " 6.15848211e-02 1.27427499e-01 2.63665090e-01 5.45559478e-01\n",
      " 1.12883789e+00 2.33572147e+00 4.83293024e+00 1.00000000e+01]\n"
     ]
    }
   ],
   "source": [
    "# Create a range of values to test for the parameter C\n",
    "cvalues_list = np.logspace(-5, 1, 20)\n",
    "print(cvalues_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=Fals...\n",
       "       1.83298071e-04, 3.79269019e-04, 7.84759970e-04, 1.62377674e-03,\n",
       "       3.35981829e-03, 6.95192796e-03, 1.43844989e-02, 2.97635144e-02,\n",
       "       6.15848211e-02, 1.27427499e-01, 2.63665090e-01, 5.45559478e-01,\n",
       "       1.12883789e+00, 2.33572147e+00, 4.83293024e+00, 1.00000000e+01])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize cvalue\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', solver=\"lbfgs\")\n",
    "param_grid = {'C': cvalues_list\n",
    "             }\n",
    "reg_l2_opt_pca = GridSearchCV(classifier, \n",
    "                                param_grid, \n",
    "                                cv=folds_regr,\n",
    "                                scoring=\"neg_mean_squared_log_error\",\n",
    "                                n_jobs=-1)     \n",
    "reg_l2_opt_pca.fit(X_train_projected, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score  -0.9289863491575551\n",
      "Scaled, l2 regularization (C=6.16e-02): rmsle = 0.964\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=reg_l2_opt_pca.best_score_\n",
    "print(\"best_score \", best_score)\n",
    "# optimal value of C\n",
    "cvalue_opt = dict(reg_l2_opt_pca.best_estimator_.get_params(deep=False).items())['C']\n",
    "rmsle_logreg_l2_opt_pca=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled, l2 regularization (C=%.2e): rmsle = %.3f\" % (cvalue_opt, \n",
    "                                                               rmsle_logreg_l2_opt_pca))\n",
    "\n",
    "logreg_l2_opt_pca=reg_l2_opt_pca.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the score is slightly better now, but let's see if we can find a better model. For this we will try some non parametric model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let's try the <b>SVM</b> (support vector Machine) with a <b>Radial Basis Function</b> (RBF) kernel. Since it's a regression problem we will use the fonction SVR(support vector regression) instead of SVC(support vector classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just try the model with random or default hyperparameters to see how it behave. We will thune them later with a search grid cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "svr_pca=SVR(C=1.0, kernel=\"rbf\", gamma=1e-2)\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(svr_pca, X_train_projected, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9346015447653586\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_svr_pca=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_svr_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the score above, even if the hyperparameters are choosen randomly, the score is better that the ridge regression which is a good indicator. Let's see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choisir 6 valeurs pour C, entre 1e-2 et 1e3\n",
    "C_range = np.logspace(-2, 3, 6)\n",
    "\n",
    "# choisir 5 valeurs pour gamma, entre 1e-3 et 10\n",
    "gamma_range = np.logspace(-3, 1, 6)\n",
    "\n",
    "# choisir 6 valeurs pour epsilon, entre 1e-3 et 100\n",
    "epsilon_range = np.logspace(-3, 2, 6)\n",
    "\n",
    "# grille de paramètres\n",
    "parameters = {'C': C_range, 'gamma': gamma_range, 'epsilon':epsilon_range}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,\n",
       "                           epsilon=0.1, gamma='auto_deprecated', kernel='rbf',\n",
       "                           max_iter=-1, shrinking=True, tol=0.001,\n",
       "                           verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'epsilon': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]),\n",
       "                         'gamma': array([1.00000000e-03, 6.30957344e-03, 3.98107171e-02, 2.51188643e-01,\n",
       "       1.58489319e+00, 1.00000000e+01])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a model\n",
    "svr_rbf_pca = SVR(kernel='rbf')\n",
    "\n",
    "# initialize cross validation\n",
    "clf = GridSearchCV(estimator=svr_rbf_pca, \n",
    "                   param_grid=parameters,\n",
    "                   cv=folds_regr,\n",
    "                   scoring=\"neg_mean_squared_log_error\",\n",
    "                   n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "clf.fit(X_train_projected, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score  -0.7756685443332929\n",
      "Scaled reduced, SVM rbf kernel (C=1.00e+03), (gamma=0.040), (epsilon=100.000): rmsle = 0.881\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=clf.best_score_\n",
    "print(\"best_score \", best_score)\n",
    "# optimal value of C\n",
    "cvalue_opt = dict(clf.best_estimator_.get_params(deep=False).items())['C']\n",
    "gamma_opt = dict(clf.best_estimator_.get_params(deep=False).items())['gamma']\n",
    "epsilon_opt = dict(clf.best_estimator_.get_params(deep=False).items())['epsilon']\n",
    "rmsle_svr_rbf_opt_pca=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled reduced, SVM rbf kernel (C=%.2e), (gamma=%.3f), (epsilon=%.3f): rmsle = %.3f\" % (cvalue_opt, \n",
    "                                                gamma_opt,\n",
    "                                                epsilon_opt,\n",
    "                                                rmsle_svr_rbf_opt_pca))\n",
    "regr_svr_rbf_opt_pca=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well as shown above the grid search allow us to select better hyperparameters for the model, we even reach a higher score than before. for now it's the best candidate we have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's continue here with the <b>SVM</b> with <b>linear kernel</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#initialize a model\n",
    "svr_pca=SVR(C=1.0, kernel=\"linear\")\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(svr_pca, X_train_projected, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9035975537991217\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_svr_lin_pca=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_svr_lin_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is ok. Let's try to improve it by tuning <b>C</b> and <b>epsilon</b> with a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choisir 10 valeurs pour C, entre 1e-2 et 1e3\n",
    "C_range = np.logspace(-2, 3, 10)\n",
    "\n",
    "# choisir 10 valeurs pour epsilon, entre 1e-2 et 10\n",
    "epsilon_range = np.logspace(-3, 1, 10)\n",
    "# grille de paramètres\n",
    "parameters = {'C': C_range, 'epsilon':epsilon_range}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,\n",
       "                           epsilon=0.1, gamma='auto_deprecated',\n",
       "                           kernel='linear', max_iter=-1, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': array([1.00000000e-02, 3.59381366e-02, 1.29154967e...\n",
       "       1.66810054e+00, 5.99484250e+00, 2.15443469e+01, 7.74263683e+01,\n",
       "       2.78255940e+02, 1.00000000e+03]),\n",
       "                         'epsilon': array([1.00000000e-03, 2.78255940e-03, 7.74263683e-03, 2.15443469e-02,\n",
       "       5.99484250e-02, 1.66810054e-01, 4.64158883e-01, 1.29154967e+00,\n",
       "       3.59381366e+00, 1.00000000e+01])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a model\n",
    "svr_lin_pca = SVR(kernel='linear')\n",
    "\n",
    "# initialize cross validation\n",
    "clf = GridSearchCV(estimator=svr_lin_pca, \n",
    "                   param_grid=parameters,\n",
    "                   cv=folds_regr,\n",
    "                   scoring=\"neg_mean_squared_log_error\",\n",
    "                    n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "clf.fit(X_train_projected, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled reduced, SVM linear (C=1000.00), (epsilon=0.167) rmsle=0.8973\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=clf.best_score_\n",
    "\n",
    "# optimal value of C\n",
    "c_value_opt = dict(clf.best_estimator_.get_params(deep=False).items())['C']\n",
    "epsilon_opt=dict(clf.best_estimator_.get_params(deep=False).items())['epsilon']\n",
    "\n",
    "rmsle_svr_lin_opt_pca=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled reduced, SVM linear (C=%.2f), (epsilon=%.3f) rmsle=%.4f\" % (c_value_opt, \n",
    "                                                epsilon_opt,\n",
    "                                                rmsle_svr_lin_opt_pca))\n",
    "regr_svr_lin_opt_pca=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is again slightly different from the SVM with an <b>exponential kernel (rbf)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue our inspectation with the <b>random forest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# initialize a model\n",
    "rf=RandomForestRegressor(n_jobs=-1, n_estimators=80, max_features=0.5, min_samples_leaf=30 )\n",
    "\n",
    "#cross validate the model\n",
    "score=cross_val_score(rf, X_train_projected, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0421383790530816\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_rf_pca=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_rf_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunatly here the results are lower than expected. Let's try if we can do better by tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators='warn', n_jobs=-1,\n",
       "                                             oob_score=False, random_state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_features': [0.5], 'min_samples_leaf': [30],\n",
       "                         'n_estimators': [800]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees to use\n",
    "list_n_trees = [500, 800, 1000, 1500]\n",
    "max_features = [0.2, 0.3, 0.5, 0.7, 0.8]\n",
    "list_min_sample_leaf= [10, 30, 50, 70]\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "param_grid ={'n_estimators': list_n_trees, 'max_features':max_features, 'min_samples_leaf':list_min_sample_leaf }\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a random forest with these parameters.\n",
    "regr_rf = GridSearchCV(RandomForestRegressor(n_jobs=-1), \n",
    "                                    param_grid=param_grid,\n",
    "                                    scoring='neg_mean_squared_log_error',\n",
    "                                    cv=folds_regr,\n",
    "                                    n_jobs=-1)\n",
    "# run the cross validation using train dataset\n",
    "regr_rf.fit(X_train_projected, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score  -1.0953312250306808\n",
      "Scaled reduced, RF (nb_trees=8.00e+02), (max_features=0.500), (min_sample_leaf=30.000): rmsle = 1.047\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=regr_rf.best_score_\n",
    "print(\"best_score \", best_score)\n",
    "# optimal value of C\n",
    "n_trees_opt = dict(regr_rf.best_estimator_.get_params(deep=False).items())['n_estimators']\n",
    "max_features_opt = dict(regr_rf.best_estimator_.get_params(deep=False).items())['max_features']\n",
    "min_sample_leaf_opt = dict(regr_rf.best_estimator_.get_params(deep=False).items())['min_samples_leaf']\n",
    "rmsle_rf_opt_pca=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled reduced, RF (nb_trees=%.2e), (max_features=%.3f), (min_sample_leaf=%.3f): rmsle = %.3f\" % (n_trees_opt, \n",
    "                                                max_features_opt,\n",
    "                                                min_sample_leaf_opt,\n",
    "                                                rmsle_rf_opt_pca))\n",
    "regr_rf_opt_pca=rf_best=regr_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before the results are not that good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the  <b>Bagging trees</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's explore a simple <b>Bagging trees model</b> using a simple <b>SVM as a base estimator</b>. We will set up the hyperparameters later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "regr_br_pca = BaggingRegressor(base_estimator=SVR(kernel='rbf'), n_estimators=300, max_features=0.3, max_samples=0.2, n_jobs=-1, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(regr_br_pca, X_train_projected, y_tr, scoring=\"neg_mean_squared_log_error\", cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935596181079028\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_br_pca=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_br_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to tune our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=BaggingRegressor(base_estimator=SVR(C=1.0,\n",
       "                                                           cache_size=200,\n",
       "                                                           coef0=0.0, degree=3,\n",
       "                                                           epsilon=0.1,\n",
       "                                                           gamma='auto_deprecated',\n",
       "                                                           kernel='rbf',\n",
       "                                                           max_iter=-1,\n",
       "                                                           shrinking=True,\n",
       "                                                           tol=0.001,\n",
       "                                                           verbose=False),\n",
       "                                        bootstrap=True,\n",
       "                                        bootstrap_features=False,\n",
       "                                        max_features=1.0, max_samples=1.0,\n",
       "                                        n_estimators=10, n_jobs=-1,\n",
       "                                        oob_score=False, random_state=0,\n",
       "                                        verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_features': [0.1], 'max_samples': [0.8],\n",
       "                         'n_estimators': [30]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_log_error', verbose=0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees to use\n",
    "list_n_trees = [30, 40, 50, 60, 70] \n",
    "max_features = [0.1, 0.3, 0.5, 0.7]\n",
    "max_samples= [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "param_grid ={'n_estimators': list_n_trees, 'max_features':max_features, 'max_samples':max_samples}\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a bagging trees with these parameters.\n",
    "regr_br = GridSearchCV(BaggingRegressor(SVR(kernel='rbf'), n_jobs=-1, random_state=0), \n",
    "                                    param_grid=param_grid,\n",
    "                                    scoring='neg_mean_squared_log_error',\n",
    "                                    cv=folds_regr,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "regr_br.fit(X_train_projected, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled reduced, BR (nb_trees=3.00e+01), (max_features=0.100), (max_samples=0.800), (base_estimator=SVR): rmsle = 0.933\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=regr_br.best_score_\n",
    "\n",
    "# optimal value of C\n",
    "n_trees_opt = dict(regr_br.best_estimator_.get_params(deep=False).items())['n_estimators']\n",
    "max_features_opt = dict(regr_br.best_estimator_.get_params(deep=False).items())['max_features']\n",
    "max_samples_opt = dict(regr_br.best_estimator_.get_params(deep=False).items())['max_samples']\n",
    "base_estimator_opt=dict(regr_br.best_estimator_.get_params(deep=False).items())['base_estimator']\n",
    "\n",
    "rmsle_br_opt_pca=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled reduced, BR (nb_trees=%.2e), (max_features=%.3f), (max_samples=%.3f), (base_estimator=%.3s): rmsle = %.3f\" % (n_trees_opt, \n",
    "                                                max_features_opt,\n",
    "                                                max_samples_opt,\n",
    "                                                base_estimator_opt,                                                                    \n",
    "                                                rmsle_br_opt_pca))\n",
    "regr_br_opt_pca=regr_br.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are compared to the one before the PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happen with <b>MARS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "model_mars_pca = Earth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#evaluation of the model\n",
    "score = cross_val_score(regr_br_pca, X_train_projected, y_tr, scoring=make_scorer(custom_rmsle_error, greater_is_better=False), cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9360420526198067\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_mars_pca=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_mars_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before I didn't tune the hyperparameters for the same reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For last the <b>gradient Boosting algorithm</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "regr_gb_pca = GradientBoostingRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# compute the score \n",
    "score = cross_val_score(regr_br_pca, X_train_projected, y_tr, scoring=make_scorer(custom_rmsle_error, greater_is_better=False), cv=folds_regr, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9351594847741964\n"
     ]
    }
   ],
   "source": [
    "#rmsle score of the model\n",
    "rmsle_gb_pca=math.sqrt(abs(np.average(score)))\n",
    "\n",
    "print(rmsle_gb_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We runned first the the algorithm with the default value and we got a bad score. let's see if we can do better with the hyperparameters tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingRegressor(alpha=0.9,\n",
       "                                                 criterion='friedman_mse',\n",
       "                                                 init=None, learning_rate=0.1,\n",
       "                                                 loss='ls', max_depth=3,\n",
       "                                                 max_features=None,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_spl...\n",
       "                                                 verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.15], 'max_depth': [2],\n",
       "                         'max_features': [0.1], 'min_samples_leaf': [5],\n",
       "                         'min_weight_fraction_leaf': [0], 'n_estimators': [500],\n",
       "                         'subsample': [0.001]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(custom_rmsle_error, greater_is_better=False),\n",
       "             verbose=0)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees to use\n",
    "list_n_trees = [100, 300, 500, 700, 900, 1000] \n",
    "max_features = [0.1, 0.3, 0.5, 0.7]\n",
    "min_samples_leaf= [1, 2, 5, 10]\n",
    "max_depth= [2, 3, 4]\n",
    "subsample= [0.001, 0.01, 0.1]\n",
    "min_weight_fraction_leaf= [0, 0.1, 0.15, 0.2]\n",
    "learning_rate= [ 0.01, 0.05 ,0.1, 0.15]\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "param_grid ={'n_estimators': list_n_trees, 'max_features':max_features, 'min_samples_leaf':min_samples_leaf, 'max_depth':max_depth, 'subsample': subsample, 'min_weight_fraction_leaf': min_weight_fraction_leaf, 'learning_rate': learning_rate}\n",
    "\n",
    "# Initialize a GridSearchCV object that will be used to cross-validate\n",
    "# a bagging trees with these parameters.\n",
    "regr_gb = GridSearchCV(GradientBoostingRegressor(random_state=0), \n",
    "                                    param_grid=param_grid,\n",
    "                                    scoring=make_scorer(custom_rmsle_error, greater_is_better=False),\n",
    "                                    cv=folds_regr,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "# run the cross validation using train dataset\n",
    "regr_gb.fit(X_train_projected, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled reduced, GB (nb_trees=5.00e+02), (max_features=0.100), (min_samples_leaf_opt=5.000), (max_depth_opt=2.000), (subsample_opt=0.001000), (min_weight_fraction_leaf=0.000000), (learning_rate=0.150) : rmsle = 1.251\n"
     ]
    }
   ],
   "source": [
    "# get mean scores of the best model\n",
    "best_score=regr_gb.best_score_\n",
    "\n",
    "# optimal value of C\n",
    "n_trees_opt = dict(regr_gb.best_estimator_.get_params(deep=False).items())['n_estimators']\n",
    "max_features_opt = dict(regr_gb.best_estimator_.get_params(deep=False).items())['max_features']\n",
    "min_samples_leaf_opt = dict(regr_gb.best_estimator_.get_params(deep=False).items())['min_samples_leaf']\n",
    "max_depth_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['max_depth']\n",
    "subsample_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['subsample']\n",
    "min_weight_fraction_leaf_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['min_weight_fraction_leaf']\n",
    "learning_rate_opt=dict(regr_gb.best_estimator_.get_params(deep=False).items())['learning_rate']\n",
    "\n",
    "\n",
    "\n",
    "rmsle_gb_opt_pca=math.sqrt(abs(best_score))\n",
    "\n",
    "print(\"Scaled reduced, GB (nb_trees=%.2e), (max_features=%.3f), (min_samples_leaf_opt=%.3f), (max_depth_opt=%.3f), (subsample_opt=%f), (min_weight_fraction_leaf=%f), (learning_rate=%.3f) : rmsle = %.3f\" % \n",
    "                                                (n_trees_opt, \n",
    "                                                max_features_opt,\n",
    "                                                min_samples_leaf_opt,\n",
    "                                                max_depth_opt,\n",
    "                                                subsample_opt,\n",
    "                                                min_weight_fraction_leaf_opt,\n",
    "                                                learning_rate_opt,\n",
    "                                                rmsle_gb_opt))\n",
    "regr_gb_opt_pca=regr_gb.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We' ve got a slightly better result but nothing special. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's resume our result in a dataframe shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>RMSLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge logistic reg</td>\n",
       "      <td>{'C': 0.06158482110660261}</td>\n",
       "      <td>0.963839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM-rbf</td>\n",
       "      <td>{'C': 1000.0, 'gamma': 0.039810717055349734, '...</td>\n",
       "      <td>0.880720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM-linear</td>\n",
       "      <td>{'C': 1000.0, 'epsilon': 0.1668100537200059}</td>\n",
       "      <td>0.897284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_features': 0.5, 'n_estimators': 800, 'mi...</td>\n",
       "      <td>1.046581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>{'base_estimator': SVR(C=1.0, cache_size=200, ...</td>\n",
       "      <td>0.932612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MARS</td>\n",
       "      <td>{'max_degree': [None, 'default: 1'], 'max_term...</td>\n",
       "      <td>0.936226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>{'n_estimators': 500, 'max_features': 0.1, 'mi...</td>\n",
       "      <td>1.133375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model                                         Parameters  \\\n",
       "0  Ridge logistic reg                         {'C': 0.06158482110660261}   \n",
       "1             SVM-rbf  {'C': 1000.0, 'gamma': 0.039810717055349734, '...   \n",
       "2          SVM-linear       {'C': 1000.0, 'epsilon': 0.1668100537200059}   \n",
       "3       Random Forest  {'max_features': 0.5, 'n_estimators': 800, 'mi...   \n",
       "4             Bagging  {'base_estimator': SVR(C=1.0, cache_size=200, ...   \n",
       "5                MARS  {'max_degree': [None, 'default: 1'], 'max_term...   \n",
       "6   Gradient Boosting  {'n_estimators': 500, 'max_features': 0.1, 'mi...   \n",
       "\n",
       "      RMSLE  \n",
       "0  0.963839  \n",
       "1  0.880720  \n",
       "2  0.897284  \n",
       "3  1.046581  \n",
       "4  0.932612  \n",
       "5  0.936226  \n",
       "6  1.133375  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['Ridge logistic reg',{'C': logreg_l2_opt_pca.get_params(deep=False)['C']}, rmsle_logreg_l2_opt_pca],\n",
    "        ['SVM-rbf',{'C': regr_svr_rbf_opt_pca.get_params(deep=False)['C'], 'gamma': regr_svr_rbf_opt_pca.get_params(deep=False)['gamma'], 'epsilon': regr_svr_rbf_opt_pca.get_params(deep=False)['epsilon'] }, rmsle_svr_rbf_opt_pca],\n",
    "        ['SVM-linear',{'C': regr_svr_lin_opt_pca.get_params(deep=False)['C'], 'epsilon': regr_svr_lin_opt_pca.get_params(deep=False)['epsilon']}, rmsle_svr_lin_opt_pca],\n",
    "        ['Random Forest',{'max_features': regr_rf_opt_pca.get_params(deep=False)['max_features'], 'n_estimators': regr_rf_opt_pca.get_params(deep=False)['n_estimators'], 'min_samples_leaf': regr_rf_opt_pca.get_params(deep=False)['min_samples_leaf'] }, rmsle_rf_opt_pca],\n",
    "        ['Bagging',{'base_estimator': regr_br_opt_pca.get_params(deep=False)['base_estimator'], 'max_features': regr_br_opt_pca.get_params(deep=False)['max_features'], 'n_estimators': regr_br_opt_pca.get_params(deep=False)['n_estimators'], 'max_samples': regr_br_opt_pca.get_params(deep=False)['max_samples'] }, rmsle_br_opt_pca],\n",
    "        ['MARS', {'max_degree': [model_mars_pca.get_params(deep=False)['max_degree'] ,\"default: 1\"] , 'max_terms': [model_mars_pca.get_params(deep=False)['max_terms'], \"default: min(2 * nb_features + nb_rows // 10, 400)\"]}, rmsle_mars_pca ],\n",
    "        ['Gradient Boosting',{'n_estimators': regr_gb_opt_pca.get_params(deep=False)['n_estimators'], 'max_features': regr_gb_opt_pca.get_params(deep=False)['max_features'], 'min_samples_leaf': regr_gb_opt_pca.get_params(deep=False)['min_samples_leaf'],  'max_depth': regr_gb_opt_pca.get_params(deep=False)['max_depth'], 'subsample': regr_gb_opt_pca.get_params(deep=False)['subsample'], 'min_weight_fraction_leaf': regr_gb_opt_pca.get_params(deep=False)['min_weight_fraction_leaf'], 'learning_rate': regr_gb_opt_pca.get_params(deep=False)['learning_rate']}, rmsle_gb_opt_pca]]\n",
    "df_pca = pd.DataFrame(data,columns=['model','Parameters', 'RMSLE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_pca' (DataFrame)\n",
      "Stored 'df' (DataFrame)\n",
      "Stored 'logreg_l2_opt_pca' (LogisticRegression)\n",
      "Stored 'regr_svr_rbf_opt_pca' (SVR)\n",
      "Stored 'regr_svr_lin_opt_pca' (SVR)\n",
      "Stored 'regr_rf_opt_pca' (RandomForestRegressor)\n",
      "Stored 'regr_br_opt_pca' (BaggingRegressor)\n",
      "Stored 'model_mars_pca' (Earth)\n",
      "Stored 'regr_gb_opt_pca' (GradientBoostingRegressor)\n",
      "Stored 'logreg_l2_opt' (LogisticRegression)\n",
      "Stored 'regr_svr_rbf_opt' (SVR)\n",
      "Stored 'regr_svr_lin_opt' (SVR)\n",
      "Stored 'regr_rf_opt' (RandomForestRegressor)\n",
      "Stored 'regr_br_opt' (BaggingRegressor)\n",
      "Stored 'model_mars' (Earth)\n",
      "Stored 'regr_gb_opt' (GradientBoostingRegressor)\n"
     ]
    }
   ],
   "source": [
    "%store df_pca\n",
    "%store df\n",
    "\n",
    "%store logreg_l2_opt_pca\n",
    "%store regr_svr_rbf_opt_pca\n",
    "%store regr_svr_lin_opt_pca\n",
    "%store regr_rf_opt_pca\n",
    "%store regr_br_opt_pca\n",
    "%store model_mars_pca\n",
    "%store regr_gb_opt_pca\n",
    "\n",
    "%store logreg_l2_opt\n",
    "%store regr_svr_rbf_opt\n",
    "%store regr_svr_lin_opt\n",
    "%store regr_rf_opt\n",
    "%store regr_br_opt\n",
    "%store model_mars\n",
    "%store regr_gb_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result our best model is the <b>SVM with an rbf kernel</b> and it's the one we will for our kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our best model on our complete training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train our model on all the data set\n",
    "regr_svr_rbf_opt_pca.fit(X_train_projected, y_tr)\n",
    "\n",
    "#generate a prediction: the floor is to remove the float since the predictions must be integers\n",
    "y_pred_test_svr_rbf_opt_pca=floor(regr_svr_rbf_opt_pca.predict(X_test_projected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2084.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1852.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2872.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1379.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction\n",
       "Id              \n",
       "0         1676.0\n",
       "1         1706.0\n",
       "2         1257.0\n",
       "3         2084.0\n",
       "4         1852.0\n",
       "...          ...\n",
       "1995      2872.0\n",
       "1996      1331.0\n",
       "1997      2021.0\n",
       "1998      1440.0\n",
       "1999      1379.0\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save our result on a data frame\n",
    "y_result = pd.DataFrame(y_pred_test_svr_rbf_opt_pca,columns=['Prediction'])\n",
    "y_result.index.name = 'Id'\n",
    "y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our predictions in a file \n",
    "y_result.to_csv('data/prediction_svm_rbf_pca.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the file we will use for the kaggle test. according to the results of the test we may choose the same model we got before the PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's save our models and results to avoid loosing them if the server restart (it happenend many times and reruning everything is masive lose of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/results.bin\",\"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "    pickle.dump(df_pca, f)\n",
    "\n",
    "\n",
    "with open(\"data/binary_models.bin\",\"wb\") as f:\n",
    "    pickle.dump(logreg_l2_opt_pca, f)\n",
    "    pickle.dump(regr_svr_rbf_opt_pca, f)\n",
    "    pickle.dump(regr_svr_lin_opt_pca, f)\n",
    "    pickle.dump(regr_rf_opt_pca, f)\n",
    "    pickle.dump(regr_br_opt_pca, f)\n",
    "    pickle.dump(model_mars_pca, f)\n",
    "    pickle.dump(regr_gb_opt_pca, f)\n",
    "    pickle.dump(logreg_l2_opt, f)\n",
    "    pickle.dump(regr_svr_rbf_opt, f)\n",
    "    pickle.dump(regr_svr_lin_opt, f)\n",
    "    pickle.dump(regr_rf_opt, f)\n",
    "    pickle.dump(regr_br_opt, f)\n",
    "    pickle.dump(model_mars, f)\n",
    "    pickle.dump(regr_gb_opt, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/model_results.csv', index=True)\n",
    "df_pca.to_csv('data/model_results_pca.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
